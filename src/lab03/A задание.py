import re
from collections import Counter


def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
    if not text:
        return ""
    
    # –ó–∞–º–µ–Ω–∞ —ë –Ω–∞ –µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if yo2e:
        text = text.replace('—ë', '–µ').replace('–Å', '–ï')
    
    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ casefold –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if casefold:
        text = text.casefold()
    
    # –ó–∞–º–µ–Ω–∞ –Ω–µ–≤–∏–¥–∏–º—ã—Ö —É–ø—Ä–∞–≤–ª—è—é—â–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –ø—Ä–æ–±–µ–ª—ã
    # \t, \r, \n –∏ –¥—Ä—É–≥–∏–µ –ø—Ä–æ–±–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
    text = re.sub(r'[\t\r\n\v\f]', ' ', text)
    
    # –°—Ö–ª–æ–ø—ã–≤–∞–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –ø—Ä–æ–±–µ–ª–æ–≤ –∏ —É–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤ –ø–æ –∫—Ä–∞—è–º
    text = re.sub(r' +', ' ', text).strip()
    
    return text


def tokenize(text: str) -> list[str]:
    """–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ç–æ–∫–µ–Ω—ã"""
    if not text:
        return []
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–ª–æ–≤
    # \w+ (–±—É–∫–≤—ã/—Ü–∏—Ñ—Ä—ã/–ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏–µ) –ø–ª—é—Å —Å–ª–æ–≤–∞ —Å –¥–µ—Ñ–∏—Å–∞–º–∏ –≤–Ω—É—Ç—Ä–∏
    pattern = r'\b[\w]+(?:-[\w]+)*\b'
    tokens = re.findall(pattern, text)
    
    return tokens


def count_freq(tokens: list[str]) -> dict[str, int]:
    """–ü–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç —Ç–æ–∫–µ–Ω–æ–≤"""
    return dict(Counter(tokens))


def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:
    """–¢–æ–ø-N —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ —á–∞—Å—Ç–æ—Ç–µ"""
    if not freq:
        return []
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å–Ω–∞—á–∞–ª–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é —á–∞—Å—Ç–æ—Ç—ã, –∑–∞—Ç–µ–º –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É
    sorted_items = sorted(
        freq.items(),
        key=lambda x: (-x[1], x[0])  # - –¥–ª—è —É–±—ã–≤–∞–Ω–∏—è —á–∞—Å—Ç–æ—Ç—ã, –æ–±—ã—á–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –¥–ª—è —Å—Ç—Ä–æ–∫
    )
    
    return sorted_items[:n]


# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
if __name__ == "__main__":
    # –¢–µ—Å—Ç—ã –¥–ª—è normalize
    print("=== normalize ===")
    print(repr(normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t")))  # "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"
    print(repr(normalize("—ë–∂–∏–∫, –Å–ª–∫–∞")))  # "–µ–∂–∏–∫, –µ–ª–∫–∞"
    print(repr(normalize("Hello\r\nWorld")))  # "hello world"
    print(repr(normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  ")))  # "–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã"
    
    # –¢–µ—Å—Ç—ã –¥–ª—è tokenize
    print("\n=== tokenize ===")
    print(tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"))  # ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"]
    print(tokenize("hello,world!!!"))  # ["hello", "world"]
    print(tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ"))  # ["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"]
    print(tokenize("2025 –≥–æ–¥"))  # ["2025", "–≥–æ–¥"]
    print(tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"))  # ["emoji", "–Ω–µ", "—Å–ª–æ–≤–æ"]
    
    # –¢–µ—Å—Ç—ã –¥–ª—è count_freq + top_n
    print("\n=== count_freq + top_n ===")
    tokens1 = ["a", "b", "a", "c", "b", "a"]
    freq1 = count_freq(tokens1)
    print(freq1)  # {"a":3,"b":2,"c":1}
    print(top_n(freq1, 2))  # [("a",3), ("b",2)]
    
    tokens2 = ["bb", "aa", "bb", "aa", "cc"]
    freq2 = count_freq(tokens2)
    print(freq2)  # {"aa":2,"bb":2,"cc":1}
    print(top_n(freq2, 2))  # [("aa",2), ("bb",2)]